\chapter{Introduction}
\label{c:intro}

% Background, Motivation and Relevant work
Attention plays an important role in human vision. For example, when
we look at an image, our eye movements comprise a succession of {\em
fixations} (repetitive positioning of eyes to parts of the image)
and {\em saccades} (rapid eye jump). Those parts of the image that
cause eye fixations and capture primary attention are called {\em
regions of interest} (ROIs). Studies in visual attention and eye
movement have shown that humans generally only attend to a few ROIs.
Detecting these visually attentive regions in images is challenging
but useful in many multimedia applications, such as automatic
thumbnail cropping, object recognition, content-based image
retrieval, adaptive image compression and automatic browsing in
small-screen devices.

Describe the common dilemma between exploration and exploitation for real-valued optimization algorithms.
Describe our basic assumption that problems worth solving are hierarchical decomposable~\cite{herbsimon:}.

% Thesis Objective
\section{Thesis Objectives}
We propose a technique that helps identify {\em regions of interest} (ROI) to explore and \textit{allocate resources} according to remaining evaluations.



First, describe why is it important to identify ROIs.
Describe how subspace projection creates a \textit{well-defined boundary} that some algorithms need.
Describe how subspace projection helps  solve \textit{inseparable problems} while enhance the ability to find optimium.

Second, describe how the proposed resources allocation benefits optimization.
Describe different strategies one should take given different evaluations left.
Describe how Multi-armed Bandit (MAB) algorithms are suitable for the scenario, instead of decision theory, reinforcement learning and Markov Decision Process.
MAB learns models from outcomes while the actions do not change the state of the world.

After identifying the potential uni-modals in the search space,
we also need to determine the resource to invest in each promising region in order to find the global optimum.
However, for a fixed amount of total evaluations, 
spending more time searching on one hill implies 
less attention on exploring other possible uni-modals in the search space.
This is the common delimma between \textit{exploration} and 
\textit{exploitation} for all real-valued optimization algorithms.


\section{Roadmap}
This thesis is composed of seven chapters.


\textbf{Chapter~\ref{chapter:algos}} presents three optimization algorithms that are adopted for comparisons. 
These three algorithms each have different characteristics. 
The Covariance Matrix Adaptation Evolutionary Strategy.
The Standard Particle Swarm Optimization.
The Ant Colony Optimization for Continous Domain.


\textbf{Chapter~\ref{chapter:clustering}} presents some clustering techniques that guides the construction of ROIs and later becomes the initial points for algorithms in each arm.


\textbf{Chapter~\ref{chapter:projection}} first describes four basic affine transformation: translation, rotation, scaling and shearing.
Then the projective transformation and homogeneous coordinate are presented.


\textbf{Chapter~\ref{chapter:MAB}} briefly describes some common multi-armed bandit algorithms, including ...
Then we present our new bandit techniques.
Tranditional bandit algorithms focus on minimizing regret, while our new bandit focus on the probability of getting a rank 1 result.


\textbf{Chapter~\ref{chapter:new_bandit}} gives details of our new algorithms.
First, the framework and pseudo code are given.
Then a detailed process of initialization is given in ...
% talk about ROI, what it is, why it is important

\textbf{Chapter~\ref{chapter:conclusion}} summarizes this thesis. 
The conclusion and contributions are also given.
Some further improvements and future works are also discussed at the end.


