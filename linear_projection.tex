\chapter{Linear Projection}
\label{chapter:projection} 

In this chapter, the technique of defining a region of interest that contains only one unimodel for exploitation.
Also, some algorithms, \textit{e.g.} SPSO, requires a well-defined hypercube search space with fixed-value constraints in each dimension.
Defining the projection onto subspace can be described as a model selection.
In our case, this process does not require high accuracy, yet has a strict demand on time consumption.
Combining the requirements, we use a linear projection matrix to project the original search space 
onto a subspace with feasible solutions bounded within $[0,1]$ in each dimension.

One of the advantage for using projection matrix is that for a problem with $\ell$ variables, 
a linear projection matrix on homogeneous coordinate only requires $(\ell+1)^2$ hyperparameters to be optimized.
Therefore, less hyperparameters are assigned than directly define each hyperplane borders or vertices.  
This reduces the parameters that we need to optimize and results in less time consumption during model selection.
We also use the (1+1)-ES, a fast and simple evolutionary strategy, to optimize the matrix.
It allows us to rapidly approximate a high dimensional projection matrix within given number of iterations.

Furthermore, subspace projection also gives advantage for solving \textit{inseparable problems}.
For variables that are not independent, projection allows the algorithm
to solve an easier, rotated and sheared problem on subspace, as shown in Figure~\ref{fig:Projected_ROI}.
The homogeneous projection matrix separates the original ROI into less overlapped ROI, 
which reduces redundant search and sometimes enlarges the crucial regions.

In the following sections, we first describe the canonical affine transformation.
Then we discuss how the projection matrix allows linear projection in a homogeneous coordinate.
Finally, we give details of how we design the cost function and how we utilize the (1+1)-ES to optimize the projection matrix.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{Projected_ROI}
\caption{Projecting inseparable problems onto subspace.}\label{fig:Projected_ROI}
\end{figure}

\section{Affine Transformation}
In geometry, an affine transformation preserves points, straight lines and planes.
For two affine spaces $A$ and $B$ and any pair of points $P, Q \in A$,
an affine transformation $f$ determines a linear transformation $\varphi$ that can formally defined as:
\begin{displaymath}
\overrightarrow{f(P)f(Q)} = \varphi \overrightarrow{(PQ)}.
\end{displaymath}

Here, we list some common 2D affine transformation matrix in Figure~\ref{fig:Affine_simple}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{Affine_simple}
\caption{Some common affine transformation}\label{fig:Affine_simple}
\end{figure}

\begin{enumerate}
    \item \textbf{Translation}

            Translation moves every point in the cluster by the same amount in a given direction.
            It allows us to reallocate the center of ROI to the positions with the best fitness.

    \item \textbf{Rotation}

            Rotation is a common method to make a problem inseparable, by creating dependencies among variables.
            Therefore, using a rotation matrix to rotate the problem allows us to alter the problem characteristics.
            In higher dimensions, rotating about different axis requires different form of matrix.
            Here we only demonstrate a simple two dimensional case in Figure~\ref{fig:Affine_simple}.

    \item \textbf{Scaling}

            Scaling transformation can stretch or shrink a given search space.
            Since it changes the lengths and angles, scaling is not an Euclidean transformation.
            In search space projection, it allows us to enlarge certain areas that we are interested in,
            and shink some area that we tend to ignore on the subspace.

    \item \textbf{Shearing}

            Shearing transform acts like ``pushing'' the search space in a direction parallel to a certain plane or axis.
            It changes the lengths and angles of certain area on the subspace.
            This transformation also allows us to expend certain interesting areas on subspace for a more thorough search.

\end{enumerate} 


\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Affine}
\caption{General affine transformation in 2D}\label{fig:Affine}
\end{figure}

The general affine transformation can be expressed as the form in Figure~\ref{fig:Affine}.
All the common affine transformation matrix mentioned above, including translation, rotation, scaling and shearing can fit into this form.
The general affine transformation can be seen as the matrix product of several affine transformation matrices.
Although the affine transformation does not preserve the lengths and angles of certain shape, it does not alter the degree of a polynomial.
That means parallel lines and planes are transformed to parallel lines and planes, 
and intersecting lines and planes are also transformed into intersecting lines and planes.
It preserved certain geometric characteristics, which is beneficial for us to do searching on subspace.


However, the affine transformation can only create a hyperparallelepid ROI, as shown in Figure~\ref{fig:Affine_vs_Projective}
The parallelogram shape is sometimes not flexible enough to define a non-overlapping ROI that matches the shape of the fitness landscape.
In order to acquire a more flexible quadrilateral ROI, we need the projective transformation.
Projective transformation is more expressive than affine transformation since the last row does not have to be 0, 0, and 1.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{Affine_vs_Projective}
\caption{Affine transformation and projective transformation comparison.}\label{fig:Affine_vs_Projective}
\end{figure} 


\section{Projective Transformation}

Projective transformations are the most general linear transformations and require the use of \textit{homogeneous coordinates}.
The \textit{homogeneous coordinate} is also called \textit{projective coordinates}, 
since it is a system of coordinates widely adopted in projective geometry.
Given a point $(x,y)$ on the Euclidean plane, 
than $(xZ, yZ, Z)$ is called a set of homogeneous coordinates for the point, for any non-zero real number $Z$.
One of the great advantage of homogeneous coordinate is that the points, 
including the ones at infinity, can be represented with simpler, finite coordinates.
That is, when $Z$ is $0$, the homogeneous coordinates of the point is defined at infinity.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{Projective_matrix}
\caption{Projective transformation matrix.}\label{fig:Projective_matrix}
\end{figure} 

Therefore, as shown in Figure~\ref{fig:Projective_matrix},
for given a position $(x,y)$ on the original search space, 
we get a projected homogeneous coordinate $(x'', y'')$ on the subsapce.  
We can generalize this to any $d$-dimensional space.
The \textit{homogeneous convention} is to convert the projected position, a vector with $(d+1)$ variables, to a $d$ variable vector 
by dividing first $d$ elements with the last element.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{Inverse_transform}
\caption{Comparison between inverse affine transform and inverse projective transformation.}\label{fig:Inverse_transform}
\end{figure} 

However, unlike the affine transformation, the projective transformation does not guarantee inverse transform.
The projection matrix does not guarantee a non-zero matrix determinant.
Also, after inverse transform, 
there could be an error $(e_x, e_y)$ that deviates from the original position $(x, y)$, as shown in Figure~\ref{fig:Inverse_transform}.
Therefore, we need to \textit{optimize} the projection matrix in order to acquire a more suitable ROI, 
as well as small reconstruction errors for each samples.





\section{Optimization for Projection Matrix}

After identifying different clusters of samples, which represents a unimodal, 
we would like to further define a reasonably good ROI.
In order to obtain a nice ROI for a problem with $D$ variables, 
we need to optimize the $(D+1)^2$ parameters in the projection matrix with a loss function.
The loss function should cost little computational time and should suggest the right direction for optimization, 
in order to match the restrictions of a well-defined ROI.
Also, the optimization only needs to define a reasonably well ROI for the algorithms to search.  
In the later iterations, the matrix update procedure allows us to define a more accurate ROI with a better insight of the unimodal subproblem.
In our case, it is acceptable to sacrifice a bit of accuracy for computational time in hyperparameters tuning.  
We will describe the design of our loss function and the advantage of using a (1+1)-ES to optimize the matrix in the following sections.  


\subsection{Loss function}

The goal of the ROI is to separate the search space into non-overlapping subspaces, 
each containing an approximately normalized fitness hill.
In our loss function, we considered the following features:
\begin{enumerate}
    \item Distances to the boundary for the \textbf{points within the cluster} yet excluded in the ROI.
    \item Distances to the boundary for the \textbf{random samples within other ROIs} yet included in the ROI.
    \item Distances to the boundary for the \textbf{random samples} in the subspace that are outside of the original search space boundary.
    \item Distance of the weighted \textbf{mean} to the center of subspace $[0.5]^D$
    \item The difference for each element in the weighted \textbf{covariance matrix} to a scaled identical matrix
    \item The sum of \textbf{reconstruction} error for each particle 
\end{enumerate} 
We add up all the errors mentioned above and try to minimize that error. 
Following part explains the reason for considering each features.

First, all the particles that belong to this cluster should be kept in the boundary $[0,1]^D$.
It can easily be done by projecting all points within the cluster onto the subspace, 
and check whether if any of the projected position is outside of the boundary $[0,1]^D$.

Second, we would like to avoid ROIs from overlapping by adopting the Monte Carlo method for region collision detection. 
When optimizing one ROI, we randomly generate a given amount of samples within the subspace for all the other ROIs.
Then, we use the inverse projection matrix of each other ROIs to project these sample points back to the original search space.
After that, we use the projection matrix of the ROI that we are optimizing to project all the exterior samples onto the subspace.
This way, we can easily discover which exterior samples are within the $[0, 1]^D$ boundary.
Figure~\ref{fig:Sample_projection} shows a screen shot of the red ROI before and after optimizing the projection matrix of the red ROI.
We randomly generates 200 samples within the green and blue ROI and tries to exclude them in the red ROI.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{Sample_projection}
\caption{Snapshot of optimization for the projection matrix of the red ROI.}\label{fig:Sample_projection}
\end{figure}

Third, the Monte Carlo method described above is adopted to make sure that the ROI does not exceed the boundaries of the orignal subspace.
We randomly generate samples from the subspace and inverse project it back to the search space to make sure that all points are within boundaries.
If not, we would like to minimize the distance of sample points to the nearest border.

Forth, each underlying model for each subproblem is desirable 
to be a multivariate Gaussian distribution with the mean around the center $[0.5]^D$.
If the algorithm searches near the boundary, it indicates that ROI should be updated since the optimum solution might be outside of the border.
Placing the optimum solution at the center of the search space creates a more stable model for the algorithm to search.
This allows a thorough search around the center, and decreases the frequency of ROI update.
Therefore, we calculate the weighted mean of the points within the cluster on the subspace.
Then we minimize the distance of weighted mean to the center.  

Fifth, the weighted covariance matrix of the underlying model is also prefered to be approximately $0.2I$, where $I$ is an identical matrix.
This enables the projection matrix to rotate, scale, and shear a fitness hill.
The normalization preprocess is a common technique to make future optimization easier.
Therefore, we calculate the difference for each element in the weighted covariance matrix and $0.2I$.  

Finally, since the we project the particle positions in a homogeneous coordinate,
the correlations between reconstructed positions on the subspace might not be identical to the original ones.
We would like to minimize the transformation error while keeping the ROI within the original boundaries.
Therefore, for each possible matrix solution, we use it to project all points in the corresponding cluster onto a subsapce.
Later, we use the inverse matrix to project all the points back to the original search space to get the reconstructed positions.
Then, we calculate the distance of all points in all dimensions between the original position and the reconstructed positions.


%Given a possible solution of the projection matrix $X = [x_1, x_2, ..., x_{(d+1)^2}]$ for a problem with $d$ variables.
%Let the corresponding projection matrix be $\textbf{A}$ with shape $(d+1) \times (d+1)$.
%We define the linear projection mapping any position $\vec{x}$ to the subspace as $T$, then
%\begin{displaymath}
%T(\vec{x}) = \textbf{A}\vec{x} 
%\end{displaymath}


%matrix $A$, we define the loss function $f$ as: 

%\begin{align*}
%f(X)                &= D_{include} + D_{exclude} + D_{boundary} + D_{center} + D_{covariance} + D_{reconstruction} \\ D_{include}         &= \\
%D_{exclude}         &= \\ 
%D_{boundary}        &= \\ 
%D_{center}          &= \\ 
%D_{covariance}      &= \\ 
%D_{reconstruction}  &= \\ 
%\end{align*}


\subsection{Optimization algorithms}

With the loss function defined in the previous section, 
we still need an optimization algorithm to optimize the projection matrix.
The difficulty for \textit{hyperparameters optimization} is to balance between speed and optimization ability.
We need the hyperparameters to define a subspace that contains the possible optimum solution while costing few computation time.

We can simplify and stabilize the optimization process by giving a fixed initial solution.
Here, the initial solution is a simple dot product of a translation matrix and a scaling matrix. 
The matrix defines a hypercube, which is bounded by the minimum and maximum value in each dimension of all positions in the cluster.
This allows the algorithm to start the search around a more preferred neighborhood.

We first tried CMA-ES to optimize the projection matrix.
However, we found that it cost too much time to optimize the projection matrix with $961$ variables in the $30$-D problem
Later, we utilize the (1+1)-ES described in Algorithm~\ref{algo:1+1ES}.
We would like to reduce the computation time by giving the (1+1)-ES only one hundred iterations for each run of three random restarts.
(1+1)-ES works well with one of the result shown in Figure~\ref{fig:Sample_projection}

Finally, the infeasible solutions that belong to the cluster yet being outside of ROI after the optimization are repaired with the following steps.
We normalize the distance between the infeasible solution and the center of ROI,
to project it onto a hypershpere centering at $[0.5]^D$ and with radius $0.5$.
This way, although the reconstructed position is not accurate on the original search space,
it is still able to indicate the approximate fitness in that direction in the subspace.
Also, our basic assumption is a unimodal hill that puts the best fitness point in the center.
Therefore, marginal points should have lower fitness and should not spend more resources to exploit.
It just needs to give us a hint of the gradient in that direction.


