\chapter{Multi-armed Bandit Algorithms}
\label{chapter:MAB}

After identifying the unimodals in Chapter~\ref{chapter:clustering} and defining a ROI for exploitation in Chapter~\ref{chapter:projection},
we still need to decide how to \textbf{allocate our resources}.
During different phases of searching, the \textit{exploration vs. exploitation} dilemma needs to be handled accordingly.
There have been many literatures considering how the algorithms converge 
and how to manipulate the searching step-size in order to find the global optimum more efficiently.
Here, we propose that different strategies should be taken according to \textit{evaluations left}.
Generally, we would like to allow more exploration in the beginning when there are abundant evaluations left.
Then, we would like to gradually increase the portion of exploitation behavior.
When there are very few evaluations left, we should concentrate on exploiting the current best hill.
Therefore, instead of letting the algorithms handle both eploration and exploitation, we propose a bandit technique to help manage the exploration, and leave the simplier subproblems for the alogrithms to exploit.

Multi-armed Bandit (MAB) Algorithms are suitable for this scenario, 
since it learns model from outcomes and the actions it takes do not change the state of the world.
Moreover, the decisions that it makes help discover more knowledge which can improve future decisions.
This matches our description for exploring fitness landscape in real-valued optimization.
However, our goal is a little bit different since we focus more about obtaining the optimum solution.
Unlike canonical MAB algorithms that minimize regrets, we wish to maximize the probability of gaining the maximum rank.

In the following sections, we'll first describe the MAB problem.
Then, we breifly introduce some MAB algorithms 
that were described in the review which Vermorel et. al. made on MAB algorithms~\cite{Vermorel:2005:MAB}.
Finally, we propose a new bandit technique that aims to maximize the probability of gaining the maximum rank.


\section{The Multi-armed Bandit Problem}

Multi-aremed Bandit (MAB) Problem was originally described by Robins in 1985~\cite{Robbins:1985:MAB}.
In this problem, a gambler has to decide which machine and how often to play in a row of slot machines, a.k.a one-armed bandits. 
When played, each machine provides a reward according to a probability distribution.
Therefore, the gambler iteratively plays one lever at each round and observes the probability of reward for each arms.
Here, the gambler is also facing the \textit{exploration vs. exploitation} tradeoff.
The problem of determining the best strategy for the gambler is called the Multi-armed Bandit problem.


The MAB problem can be more formally descirbed as 
an agent deciding which one of the $K \in \mathbb{N}_+$ arms to pull at time $t$ to receive the maximum reward.
The $K$ arms can be seen as a set of real distributions $B = {R_1, ..., R_K}$.
Let $\mu_1, ..., \mu_K$ be the mean of rewards for each arm, and $\mu^* = \max_{k} \{ \mu_k \}$ be the highest reward mean.
The \textit{regret} $\rho$ after $T$ rounds is defined as
\begin{displaymath}
\rho = T\mu^* - \sum_{t=1}^{T} r_t,
\end{displaymath}
where $r_t$ is the reward at time $t$.

The goal is to minimize the regret, which represents the expected difference between the total rewards of an optimal strategy,
and the sum of the actual rewards that have been collected.  
The reason for concerning the regret is because we would like to achieve a \textit{zero-regret strategy}.
A \textit{zero-regret strategy} is a strategy whose average regret per round $\rho / T$ tends to zero with probability $1$ 
when the number of played rounds tends to infinity.
The \textit{zero-regret strategy} are guaranteed to converge to an optimal strategy if enough of rounds are played~\cite{Vermorel:2005:MAB}.



\section{Some common MAB Algorithm}


The Price of Knowledge and Estimated Reward (POKER) strategy considers three ideas: pricing uncertainty, exploiting the lever distribution and taking into account the horizon


\section{The New Bandit Technique}


