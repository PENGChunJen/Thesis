\chapter{Multi-armed Bandit Algorithms}
\label{chapter:MAB}

After identifying the unimodals in Chapter~\ref{chapter:clustering} and defining a ROI for exploitation in Chapter~\ref{chapter:projection},
we still need to decide how to \textbf{allocate our resources}.
During different phases of searching, the \textit{exploration vs. exploitation} dilemma needs to be handled accordingly.
There have been many literatures considering how the algorithms converge 
and how to manipulate the searching step-size in order to find the global optimum more efficiently.
Here, we propose that different strategies should be taken according to \textit{evaluations left}.
Generally, we would like to allow more exploration in the beginning when there are abundant evaluations left.
Then, we would like to gradually increase the portion of exploitation behavior.
When there are very few evaluations left, we should concentrate on exploiting the current best hill.
Therefore, instead of letting the algorithms handle both eploration and exploitation, we propose a bandit technique to help manage the exploration, and leave the simplier subproblems for the alogrithms to exploit.

Multi-armed Bandit (MAB) Algorithms are suitable for this scenario, 
since it learns model from outcomes and the actions it takes do not change the state of the world.
Moreover, the decisions that it makes help discover more knowledge which can improve future decisions.
This matches our description for exploring fitness landscape in real-valued optimization.
However, our goal is a little bit different since we focus more about obtaining the optimum solution.
Unlike canonical MAB algorithms that minimize regrets, we wish to maximize the probability of gaining the maximum rank.

In the following sections, we'll first describe the MAB problem.
Then, we breifly introduce some MAB algorithms 
that were described in the review which Vermorel et. al. made on MAB algorithms~\cite{Vermorel:2005:MAB}.
Finally, we propose a new bandit technique that aims to maximize the probability of gaining the maximum rank.


\section{The Multi-armed Bandit Problem}

Multi-aremed Bandit (MAB) Problem was originally described by Robins in 1985~\cite{Robbins:1985:MAB}.
In this problem, a gambler has to decide which machine to play in a row of slot machines, a.k.a one-armed bandits, and how often to play it.
When the lever is pulled, each machine provides a reward according to a probability distribution.
Therefore, the gambler iteratively plays one lever at each round and observes the probability of reward for each arms.
Here, the gambler is also facing the \textit{exploration vs. exploitation} tradeoff.
The problem of determining the best strategy for the gambler is called the Multi-armed Bandit problem.


The MAB problem can be more formally descirbed as 
an agent deciding which one of the $K \in \mathbb{N}_+$ arms to pull at time $t$ to receive the maximum reward.
The $K$ arms can be seen as a set of real distributions $B = \{ R_1, ..., R_K \} $.
Let $\mu_1, ..., \mu_K$ be the mean of rewards for each arm, and $\mu^* = \max_{k} \{ \mu_k \}$ be the highest reward mean.
The \textit{regret} $\rho$ after $T$ rounds is defined as
\begin{displaymath}
\rho = T\mu^* - \sum_{t=1}^{T} r_t,
\end{displaymath}
where $r_t$ is the reward at time $t$.

The goal is to minimize the regret, which represents the expected difference between the total rewards of an optimal strategy,
and the sum of the actual rewards that have been collected.  
The reason for concerning the regret is because we would like to achieve a \textit{zero-regret strategy}.
A \textit{zero-regret strategy} is a strategy whose average regret per round $\rho / T$ tends to zero with probability $1$ 
when the number of played rounds tends to infinity.
The \textit{zero-regret strategy} are guaranteed to converge to an optimal strategy if enough of rounds are played~\cite{Vermorel:2005:MAB}.



\section{Some common MAB Algorithm}

The $\epsilon$-greedy strategy, first proposed by Watkins~\cite{Watkins:1989:eta}, 
is the simplest and one of the most widly used strategy to solve the MAB problem.
Given an $\epsilon \in (0,1)$,
the $\epsilon$-greedy strategy chooses a random lever with a probability $\epsilon$, 
and chooses the lever with the highest estimated mean with a probability $1 - \epsilon$.
It is also known as \textit{semi-uniform} method since it implies a uniform probability of exploration among a set levers.

The Price of Knowledge and Estimated Reward (POKER) strategy considers three ideas: pricing uncertainty, exploiting the lever distribution and taking into account the horizon


\section{The New Bandit Technique}

We propose a new bandit technique that aims to maximize the probability of aquiring the best rank in the history.
We only focus on ranks instead of real-valued fitness to generalize the case.

Let $r$ be the probability of not being able to aquire the best point.
Let $m_i$ be the one of the possible models in the arm.
Let $D$ be the model of all the observed data.
The we define the probability of not being able to aquire the best point given a set of observed data as:
\begin{equation}
P(r|D) = \sum_{i} P(r|D, m_i)P(m_i|D) = \sum_{i}P(r|m_i)P(m_i|D) 
\end{equation}\label{equation:bandit}
Therefore, we would like find a remain evaluation allocation that gives a minimum probability of not being able to aquire the best point.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{Bandit_arm}
\caption{Probability of regret.}\label{fig:Bandit_arm}
\end{figure} 

In order to calculate $P(r|D)$, we need to calculate all possible models for a arm with given rank, 
total number of points in the search space, and the number of remain evaluations, 
as shown in Figure~\ref{fig:Bandit_arm}.
The indigo area represents the observed model $D$, 
while the light blue area represents all the possible models given the number of remain evaluations.
It means that if we invest the two remain evaluations on arm 1, 
we might discover two better positions with higher ranks,
or two worse positions that gives fitness worse than the current rank $2$ particle.
That is why there are two possible positions on the top and two on the bottom of each arm.
If the aquired fitness is between the current best and worst particle, 
than the rank would only exchange with another particle in one of the arms,
so no more ``holes'' than the current ranks are needed.
We can find $12$ possible models that matches the current observation, i.e. rank $1$ is included.
Therefore, only $4$ out of the $12$ models are possible to aquire the best rank.
We assume the model is uniformly distributed, that is, aquiring different ranks share the same possibility.
We also assume that all models have the same probability to be chosen with the observed data model $D$.
We can then calculate the probability of not being able to aquire the best rank in the clusters.
To demonstrate the idea, we consider the right most blue model that contains the best and the worst possible ranks in Figure~\ref{fig:Bandit_arm}.
The probability of not aquiring the best model for that specific model $m_i$ is $P(r|m_i) = 5/6$,
while the probability for choosing that model is $P(m_i|D) = 1/12$.
Similarly, we can calculate the probability of regret for all models.


\begin{figure}
\centering
\includegraphics[width=\textwidth]{Bandit_regret}
\caption{Probability of regret for all allocation combinations.}\label{fig:Bandit_regret}
\end{figure} 
Given all the models and their corresponding probability of not aquiring the best rank, 
we can use Equation~\ref{equation:bandit} to calculate the total probability of regret given a set of remain evaluations allocation.
For example, if we decide to invest both of the remaining evaluations on the first arm,
then the probability of not aquiring the best model for the right most model would be $P(r|m_i) = (5/6)^2$,
while the probability for choosing that model is still $P(m_i|D) = 1/12$.
Therefore, we can calculate the probability of regret as illustrated in Figure~\ref{fig:Bandit_regret}.
After calculate the probability of regret all possible combinations, i.e. [2,0], [1,1], [0,2], 
we select the evaluation allocation model with the minimum regret.
In this case, it is to invest all the remaining evaluations on the first cluster.
This result satisfies our assumption that in the end with few evaluations left, 
one should focus on exploiting the current best hill.
If today there are still lots of evaluations left, 
this technique gives an even evaluation allocation that only slightly favors the current best arm.
This also satisfies our assumption that in the beginning, we should explore evenly between all possible clusters.


